@article{Wronski2019,
author = {Wronski, Bartlomiej and Garcia-Dorado, Ignacio and Ernst, Manfred and Kelly, Damien and Krainin, Michael and Liang, Chia-Kai and Levoy, Marc and Milanfar, Peyman},
title = {Handheld Multi-Frame Super-Resolution},
year = {2019},
issue_date = {July 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {4},
issn = {0730-0301},
url = {https://doi.org/10.1145/3306346.3323024},
doi = {10.1145/3306346.3323024},
abstract = {Compared to DSLR cameras, smartphone cameras have smaller sensors, which limits their spatial resolution; smaller apertures, which limits their light gathering ability; and smaller pixels, which reduces their signal-to-noise ratio. The use of color filter arrays (CFAs) requires demosaicing, which further degrades resolution. In this paper, we supplant the use of traditional demosaicing in single-frame and burst photography pipelines with a multiframe super-resolution algorithm that creates a complete RGB image directly from a burst of CFA raw images. We harness natural hand tremor, typical in handheld photography, to acquire a burst of raw frames with small offsets. These frames are then aligned and merged to form a single image with red, green, and blue values at every pixel site. This approach, which includes no explicit demosaicing step, serves to both increase image resolution and boost signal to noise ratio. Our algorithm is robust to challenging scene conditions: local motion, occlusion, or scene changes. It runs at 100 milliseconds per 12-megapixel RAW input burst frame on mass-produced mobile phones. Specifically, the algorithm is the basis of the Super-Res Zoom feature, as well as the default merge method in Night Sight mode (whether zooming or not) on Google's flagship phone.},
journal = {ACM Trans. Graph.},
month = jul,
articleno = {28},
numpages = {18},
keywords = {image processing, super-resolution, photography, computational photography},
note={\url{https://doi.org/10.1145/3306346.3323024}}
}

@article{Liba2019,
author = {Liba, Orly and Murthy, Kiran and Tsai, Yun-Ta and Brooks, Tim and Xue, Tianfan and Karnad, Nikhil and He, Qiurui and Barron, Jonathan T. and Sharlet, Dillon and Geiss, Ryan and Hasinoff, Samuel W. and Pritch, Yael and Levoy, Marc},
title = {Handheld Mobile Photography in Very Low Light},
year = {2019},
issue_date = {November 2019},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {38},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/3355089.3356508},
doi = {10.1145/3355089.3356508},
abstract = {Taking photographs in low light using a mobile phone is challenging and rarely produces pleasing results. Aside from the physical limits imposed by read noise and photon shot noise, these cameras are typically handheld, have small apertures and sensors, use mass-produced analog electronics that cannot easily be cooled, and are commonly used to photograph subjects that move, like children and pets. In this paper we describe a system for capturing clean, sharp, colorful photographs in light as low as 0.3 lux, where human vision becomes monochromatic and indistinct. To permit handheld photography without flash illumination, we capture, align, and combine multiple frames. Our system employs "motion metering", which uses an estimate of motion magnitudes (whether due to handshake or moving objects) to identify the number of frames and the per-frame exposure times that together minimize both noise and motion blur in a captured burst. We combine these frames using robust alignment and merging techniques that are specialized for high-noise imagery. To ensure accurate colors in such low light, we employ a learning-based auto white balancing algorithm. To prevent the photographs from looking like they were shot in daylight, we use tone mapping techniques inspired by illusionistic painting: increasing contrast, crushing shadows to black, and surrounding the scene with darkness. All of these processes are performed using the limited computational resources of a mobile device. Our system can be used by novice photographers to produce shareable pictures in a few seconds based on a single shutter press, even in environments so dim that humans cannot see clearly.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {164},
numpages = {16},
keywords = {low-light imaging, computational photography},
note={\url{https://doi.org/10.1145/3355089.3356508}}
}

@article{Hasinoff2016,
author = {Hasinoff, Samuel W. and Sharlet, Dillon and Geiss, Ryan and Adams, Andrew and Barron, Jonathan T. and Kainz, Florian and Chen, Jiawen and Levoy, Marc},
title = {Burst Photography for High Dynamic Range and Low-Light Imaging on Mobile Cameras},
year = {2016},
issue_date = {November 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {35},
number = {6},
issn = {0730-0301},
url = {https://doi.org/10.1145/2980179.2980254},
doi = {10.1145/2980179.2980254},
abstract = {Cell phone cameras have small apertures, which limits the number of photons they can gather, leading to noisy images in low light. They also have small sensor pixels, which limits the number of electrons each pixel can store, leading to limited dynamic range. We describe a computational photography pipeline that captures, aligns, and merges a burst of frames to reduce noise and increase dynamic range. Our system has several key features that help make it robust and efficient. First, we do not use bracketed exposures. Instead, we capture frames of constant exposure, which makes alignment more robust, and we set this exposure low enough to avoid blowing out highlights. The resulting merged image has clean shadows and high bit depth, allowing us to apply standard HDR tone mapping methods. Second, we begin from Bayer raw frames rather than the demosaicked RGB (or YUV) frames produced by hardware Image Signal Processors (ISPs) common on mobile platforms. This gives us more bits per pixel and allows us to circumvent the ISP's unwanted tone mapping and spatial denoising. Third, we use a novel FFT-based alignment algorithm and a hybrid 2D/3D Wiener filter to denoise and merge the frames in a burst. Our implementation is built atop Android's Camera2 API, which provides per-frame camera control and access to raw imagery, and is written in the Halide domain-specific language (DSL). It runs in 4 seconds on device (for a 12 Mpix image), requires no user intervention, and ships on several mass-produced cell phones.},
journal = {ACM Trans. Graph.},
month = nov,
articleno = {192},
numpages = {12},
keywords = {high dynamic range, computational photography},
note={\url{https://doi.org/10.1145/2980179.2980254}}
}

@misc{ wiki:BayerFilter,
    author = "{Wikipedia contributors}",
    title = "Bayer filter --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2020",
    url = "https://en.wikipedia.org/w/index.php?title=Bayer_filter&oldid=977384299",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Bayer_filter&oldid=977384299}",
    note = "[Online; accessed 10-September-2020]"
}
  
@misc{ wiki:Demosaicing,
    author = "{Wikipedia contributors}",
    title = "Demosaicing --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2020",
    url = "https://en.wikipedia.org/w/index.php?title=Demosaicing&oldid=935211330",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Demosaicing&oldid=935211330}",
    note = "[Online; accessed 10-September-2020]"
}

@misc{ wiki:Aliasing,
    author = "{Wikipedia contributors}",
    title = "Aliasing --- {Wikipedia}{,} The Free Encyclopedia",
    year = "2020",
    url = "https://en.wikipedia.org/w/index.php?title=Aliasing&oldid=965615959",
    howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Aliasing&oldid=965615959}",
    note = "[Online; accessed 10-September-2020]"
  }

@inproceedings{46440,
title	= {Fast Fourier Color Constancy},
author	= {Jonathan T. Barron and Yun-Ta Tsai},
year	= {2017},
booktitle	= {CVPR}
}
@inproceedings{Barron2017,
  author={J. T. {Barron} and Y. {Tsai}},
  booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, 
  title={Fast Fourier Color Constancy}, 
  year={2017},
  pages={6950-6958},
  abstract={We present Fast Fourier Color Constancy (FFCC), a color constancy algorithm which solves illuminant estimation by reducing it to a spatial localization task on a torus. By operating in the frequency domain, FFCC produces lower error rates than the previous state-of-the-art by 13-20\% while being 250-3000Ã— faster. This unconventional approach introduces challenges regarding aliasing, directional statistics, and preconditioning, which we address. By producing a complete posterior distribution over illuminants instead of a single illuminant estimate, FFCC enables better training techniques, an effective temporal smoothing technique, and richer methods for error analysis. Our implementation of FFCC runs at ~700 frames per second on a mobile device, allowing it to be used as an accurate, real-time, temporally-coherent automatic white balance algorithm.},
  keywords={error analysis;fast Fourier transforms;image colour analysis;smoothing methods;statistical distributions;FFCC;Fast Fourier Color Constancy;color constancy algorithm;illuminant estimation;spatial localization task;single illuminant estimate;frequency domain operation;aliasing;directional statistics;complete posterior distribution;temporal smoothing technique;error analysis;accurate real-time temporally-coherent automatic white balance algorithm;Image color analysis;Histograms;Computer vision;Cameras;Two dimensional displays;Standards},
  doi={10.1109/CVPR.2017.735},
  ISSN={1063-6919},
  month={July},
  note="\url{https://ieeexplore.ieee.org/document/8100218}, \url{https://research.google/pubs/pub46440/}"
}

@misc{blog:Wronski2018, title={See Better and Further with {S}uper {R}es {Z}oom on the {P}ixel 3}, url={https://ai.googleblog.com/2018/10/see-better-and-further-with-super-res.html}, journal={Google AI Blog}, publisher={Google}, author={Wronski, Bartlomiej and Milanfar, Peyman}, year={2018}, month={Oct},
note={\url{https://ai.googleblog.com/2018/10/see-better-and-further-with-super-res.html}}
}

@misc{blog:Levoy2018, title={{N}ight {S}ight: Seeing in the Dark on {P}ixel Phones}, url={https://ai.googleblog.com/2018/11/night-sight-seeing-in-dark-on-pixel.html}, journal={Google AI Blog}, publisher={Google}, author={Levoy, Marc and Pritch, Yael}, year={2018}, month={Nov},
note={\url{https://ai.googleblog.com/2018/11/night-sight-seeing-in-dark-on-pixel.html}}
}

@misc{ wiki:ToneMapping,
author = "{Wikipedia contributors}",
title = "Tone mapping --- {Wikipedia}{,} The Free Encyclopedia",
year = "2020",
url = "https://en.wikipedia.org/w/index.php?title=Tone_mapping&oldid=978774859",
howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Tone_mapping&oldid=978774859}",
note = "[Online; accessed 8-October-2020]"
}